{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "train_ds = CIFAR100(\"./data\", train=True, download=True, transform=transforms_)\n",
    "\n",
    "CLASSES = train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=60, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logdir\")\n",
    "device = \"cuda\" if pt.cuda.is_available() else \"cpu\"\n",
    "batch_size = 60\n",
    "gen_lr = 1e-4\n",
    "desc_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_n_random_dataset_samples(data, labels, n=200):\n",
    "    values = pt.randperm(len(data))\n",
    "    return data[values][:n], labels[values][:n]\n",
    "\n",
    "images, labels = selected_n_random_dataset_samples(train_ds.data, train_ds.targets)\n",
    "\n",
    "class_labels = [CLASSES[labels] for label in labels]\n",
    "\n",
    "writer.add_embedding(\n",
    "    images.view(-1, 32*32),\n",
    "    metadata=class_labels,\n",
    "    label_img=images.unsqueeze(1)\n",
    ")\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels=None):    \n",
    "\n",
    "    images = images.detach().numpy() if type(images) == pt.Tensor else images\n",
    "\n",
    "    for idx in range(0, len(images)):\n",
    "        plt.subplot(2, 10, idx+1) if len(images) > 10 else plt.subplot(1, len(images), idx+1)\n",
    "\n",
    "        plt.imshow(images[idx].T)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        if labels is not None:\n",
    "            plt.title(CLASSES[labels[idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4), dpi=300)\n",
    "\n",
    "n_samples_per_class = {\n",
    "    class_: 0 for class_ in CLASSES\n",
    "}\n",
    "\n",
    "for _, labels in train_loader:\n",
    "    for label in labels:\n",
    "        n_samples_per_class[CLASSES[label.item()]] += 1\n",
    "\n",
    "plt.xticks(rotation=90, ha=\"center\")\n",
    "plt.margins(x=0.01)\n",
    "plt.bar(n_samples_per_class.keys(), n_samples_per_class.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images = images[:20]\n",
    "labels = labels[:20]\n",
    "\n",
    "plt.figure(figsize=(20, 4), dpi=300)\n",
    "plot_images(images, labels)\n",
    "\n",
    "writer.add_images(tag=\"20 Images from Train Data Loader\", img_tensor=images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Train dataset size: {train_ds.data.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=384, out_features=384*4*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Reshaping into (384, 4, 4) the input in forward method\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2), # (384, 8, 8)\n",
    "            nn.Conv2d(in_channels=384, out_channels=512, kernel_size=5, padding=\"same\"), # (512, 8, 8)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2), # (512, 16, 16)\n",
    "            nn.Conv2d(in_channels=512, out_channels=768, kernel_size=5, padding=\"same\"), # (768, 16, 16)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2), # (768, 32, 32)\n",
    "            nn.Conv2d(in_channels=768, out_channels=1024, kernel_size=5, padding=\"same\"), # (1024, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.down_sample = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=4, padding=\"same\"),  # (512, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 3, kernel_size=4, padding=\"same\"),  # (3, 32, 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 384, 4, 4)\n",
    "        x = self.upsample(x)\n",
    "        x = self.down_sample(x)\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "generator_optimizer = Adam(generator.parameters(), lr=gen_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(pred, label):\n",
    "    return F.binary_cross_entropy(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (summary(generator.to(\"cpu\"),pt.Tensor(384, device=\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Descriminator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.__conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1), # ((32 - 3) + 2*0) / 1 + 1 = 30\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool2d(2), # Max pool kernel size is 2x2, hence 30 / 2 = 15\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),  # ((15 - 3) + 2*0) / 1 + 1 = 13\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),  # ((13 - 3) + 2*0) / 1 + 1 = 11\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool2d(2),  # Max pool kernel size is 2x2, hence 13 / 2 = 5\n",
    "        )\n",
    "        self.__linear_block = nn.Sequential(\n",
    "            nn.Linear(in_features=128*5*5, out_features=512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=512, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.__conv_block(x)\n",
    "        x = x.view(-1, 128*5*5)\n",
    "        x = self.__linear_block(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriminator = Descriminator().to(device)\n",
    "descriminator_optimizer = Adam(descriminator.parameters(), lr=desc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (summary(descriminator.to(\"cpu\"), pt.zeros((1, 3, 32, 32), device=\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriminator_loss(pred, label):\n",
    "    return F.binary_cross_entropy(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4), dpi=300)\n",
    "\n",
    "generator = generator.to(\"cpu\").eval()\n",
    "garbage = pt.from_numpy(np.random.normal(size=(4, 1, 384))).to(dtype=pt.float32, device=\"cpu\")\n",
    "\n",
    "predicted = generator(garbage)\n",
    "plot_images(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(generator, garbage)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriminator = descriminator.eval().to(device)\n",
    "predicted = descriminator(images)\n",
    "\n",
    "print (f\"Dataset image size: {images.size()}\")\n",
    "print (f\"Model Generate images: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(descriminator, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(current_epoch, total_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "\n",
    "        real_images = images.to(device)\n",
    "\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        generator = generator.to(device).eval()\n",
    "        predicted_images = generator(\n",
    "            pt.from_numpy(np.random.normal((batch_size, 1, 384))).to(dtype=pt.float32, device=device),\n",
    "        )\n",
    "\n",
    "        if True:\n",
    "            descriminator_optimizer.zero_grad()\n",
    "\n",
    "            descriminator = descriminator.to(device).train()\n",
    "            yhat_real = descriminator(real_images)\n",
    "            yhat_predicted = descriminator(predicted_images)\n",
    "\n",
    "            yhat_real_predicted = pt.concat([pt.ones_like(yhat_real), pt.zeros_like(yhat_predicted)], dim=0)\n",
    "            # real images label: 1\n",
    "            # predicted images label: 0\n",
    "\n",
    "            noise_real = 0.2 * pt.from_numpy(\n",
    "                np.random.uniform(yhat_real.shape)\n",
    "            )\n",
    "            noise_predicted = -0.2 * pt.from_numpy(\n",
    "                np.random.uniform(yhat_predicted.shape)\n",
    "            )\n",
    "            y_real_predicted = pt.concat([noise_real, noise_predicted], dim=0)\n",
    "\n",
    "            desc_loss = desc_loss(y_real_predicted, yhat_real_predicted)\n",
    "\n",
    "            desc_loss.backward()\n",
    "\n",
    "            descriminator_optimizer.step()\n",
    "        \n",
    "        generator = generator.to(device).train()\n",
    "\n",
    "        generated_images = generator(\n",
    "            pt.from_numpy(np.random.normal((batch_size, 1, 384))).to(dtype=pt.float32, device=device),\n",
    "        )\n",
    "    \n",
    "        descriminator = descriminator.to(device).eval()\n",
    "        predicted_labels = descriminator(generated_images)\n",
    "\n",
    "        gen_loss = generator_loss(\n",
    "            pt.zeros_like(predicted_labels), predicted_labels\n",
    "        )\n",
    "\n",
    "        gen_loss.backward()\n",
    "\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        print (f\"Epoch: {current_epoch+1}/{total_epochs} Descriminator loss: {desc_loss.item():.6f}\", end=\"; \")\n",
    "        print (f\"Generator Training loss: {gen_loss.item():.6f}\")\n",
    "\n",
    "        yield {\n",
    "            \"desc_loss\": desc_loss.item(),\n",
    "            \"gen_loss\": gen_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 2000\n",
    "\n",
    "desc_loss, gen_loss = [], []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    temp = next(train(epoch, total_epochs, generator, descriminator))\n",
    "\n",
    "    desc_loss.append(temp[\"desc_loss\"])\n",
    "    gen_loss.append(temp[\"gen_loss\"])\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"Generator Loss vs Descriminator Loss\",\n",
    "        {\"Generator Loss\": gen_loss, \"Descriminator Loss\": desc_loss}\n",
    "    )\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(desc_loss, label=\"Descriminator Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Descriminator Train Loss\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(gen_loss, label=\"Generator Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Descriminator Train Accuracy\")\n",
    "plt.title(\"Generator Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.save(generator, \"./generator.pt\")\n",
    "pt.save(descriminator, \"./descriminator.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
